<h1>Data Analysis and NLP</h1>
<body>
  [![Python 3.6](https://img.shields.io/badge/python-3.6-blue.svg)](https://www.python.org/downloads/release/python-360/)
  <p style="font-family: 'Lora', serif;">All the work done, code written, files created and visualizations made by me while interning at 50Hands.</p>

  <h3><b>For Emotion Recognition:</b></h3>
  <h5><b>ML & NLP</b></h5>
  <p>A program was made, using Python, that detects emotion based on textual and facial features.</p>
  <p>For textual emotion recognition, LSTM was used to train the model.</p>
  <p>For facial emotion recognition, a CNN network was used to train the model.</p>
  <p>The models were trained using Colab since it offers a GPU, hence faster computations.</p>
  
  <h3><b>For the 2020 US Elections:</b></h3>
  <h5><b>Planning, Scraping & Visualization</b></h5>
  <p>Using Tableau, visualizations were made to show important trends and statistics in relation to the 2020 US elections.</p>
  <p>Some of the trends shown are:</p>
  <ul>
    <li>Voting with respect to geography i.e., state and counties.</li>
    <li>Voting with respect to populations in all the states and counties.</li>
    <li>A state's Democratic or Republican preference with voter demographic population as a factor.</li>
    <li>A region's Democratic or Republican preference with median income and poverty level as a factor, for both, states and counties.</li>
  </ul>
  
  <h3><b>For Canada, India and USA:</b></h3>
  <h5><b>Visualization</b></h5>
  <p>Using Tableau, visualizations were made to show important Coronavirus parameters.</p>
  
  <h3><b>For Google Mobility:</b></h3>
  <h5><b>Scraping & Visualization</b></h5>
  <p>Visualization made to show the mobility data for USA at state and county level.</p>
  <p>A python script written to scrape the mobility data for USA and Canada in a orderly manner that helps in the creation of various visualizations.</p>
  <p>The main script can be found in the Mobility Data folder named Index.py</p>
  
  <h3>For Global Data:</h3>
  <h5><b>Scraping & Visualization</b></h5>
  <p>A script was written to extract global Coronavirus data from John Hopkin's University (from late January), create a data-frame and insert it into the database.</p>
  <p>The data that is inserted into the database is then used to create a Global Covid-19 Tracker, which shows the progression of cases, change in various parameters, country-wise statistics and the relation between those parameters with constants like GDP, poverty rate et cetera.</p>
  <p>The main script can be found in the 'World' folder named Index.py.</p>
  
  <h3>For Twitter Sentiment Analysis:</h3>
  <h5><b>Visualization</b></h5>
  <p>For Canada, India and USA, a visualization was made for Twitter Sentiment Analysis.</p>
  
  <h3>For Reddit Sentiment Analysis:</h3>
  <h5><b>NLP & Visualization</b></h5>
  <p>For various countries, posts were extracted from their designated Covid-19 subreddits and sentiment analysis was performed.</p>
  <p>A visualization was made showing these sentiment statistics.</p>
  <p>For Canada, India and USA, wordclouds and bigram-clouds were created using the same post extraction process as before.</p>
  <p>For this too visualizations were created.</p>
  
  <h3>For Quotes Generation:</h3>
  <h5><b>NLP</b></h5>
  <p>GPT-2 was trained using a csv of quotes, in the hope that the trained model would generate similar quotes, that made sense.</p>
  <p>Most of the code was already present and can be found in GPT-2's documentation.</p>
  <p>A few modifications were made to put the generated quotes into a CSV file, so that a quote could be extracted when the API for that is called in negligible time.</p>
  <p>For reference: <a href='https://github.com/openai/gpt-2'>https://github.com/openai/gpt-2</a></p>
  
  <br><p><b>Tools Used:</b> Tableau, Python and Google Colab.</p>
</body>
